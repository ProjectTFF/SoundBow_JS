<!DOCTYPE html>
<head>
	<meta http-equiv="content-type" content="text/html; charset=utf-8" />
	<title>SoundBow JS </title>

	<script type="text/javascript">

		// <![CDATA[
		var mobile = (/iphone|ipad|ipod|android|blackberry|mini|windows\sce|palm/i.test(navigator.userAgent.toLowerCase()));
		if (mobile) {
			document.location = "http://www.binaura.net/stc/wrx/soundbow/mobile.html";
		}
		// ]]>

		var viewportwidth;
		var viewportheight;

		// the more standards compliant browsers (mozilla/netscape/opera/IE7) use window.innerWidth and window.innerHeight

		viewportwidth = window.innerWidth,
			viewportheight = window.innerHeight

	</script>

	
	<link rel="stylesheet" href="public/stylesheets/style.css">
	<!--[if lt IE 9]>
		<script type="text/javascript">alert("Your browser does not support the canvas tag.");</script>
	<![endif]-->
	<script src="./assets/code/processing.js" type="text/javascript"></script>
	<script src="./assets/code/jQuery.js" type="text/javascript"></script>

	<script type="text/javascript">
		// convenience function to get the id attribute of generated sketch html element
		function getProcessingSketchId() { return 'SoundBowJS'; }
	</script>


	<script type="text/javascript" language="javascript">


		var audioElementArrayPentascale = new Array();
		var audioElementArrayHarmonicscale = new Array();


		for (var i = 0; i < 8; i++) {

			//	load sounds for pentaton scale

			audioElementArrayPentascale[i] = document.createElement('audio');
			var index = i + 1;
			audioElementArrayPentascale[i].setAttribute('src', './assets/sounds/' + index + '.wav');

			//	load sounds for harmonic scale

			audioElementArrayHarmonicscale[i] = document.createElement('audio');
			audioElementArrayHarmonicscale[i].setAttribute('src', './assets/sounds/1' + index + '.wav');
		}

		function PlayPentaSound(sel) {
			var playbackElement;

			console.log(audioElementArrayPentascale.length);
			console.log("selection : " + sel)
			if ((sel >= 1) && (sel <= 8)) {
				audioElementArrayPentascale[sel - 1].load;
				audioElementArrayPentascale[sel - 1].currentTime = 0;
				audioElementArrayPentascale[sel - 1].play();
			}
		}

		function PlayHarmonicSound(sel) {

			console.log(audioElementArrayHarmonicscale.length);
			console.log("selection : " + sel)
			if ((sel >= 1) && (sel <= 8)) {
				audioElementArrayHarmonicscale[sel - 1].load;
				audioElementArrayHarmonicscale[sel - 1].currentTime = 0;
				audioElementArrayHarmonicscale[sel - 1].play();
			}
		}



	</script>


	<div class="left">
		<div id="startButton" class="button">
			Start
		</div>
		<h2>Preview</h2>
		<video id="preview" width="160" height="120" autoplay muted></video>
	</div>

	<div class="right">
		<div id="stopButton" class="button">
			Stop
		</div>
		<h2>Recording</h2>
		<video id="recording" width="160" height="120" controls></video>
		<a id="downloadButton" class="button">
			Download
		</a>
	</div>

	<div class="bottom">
		<pre id="log"></pre>
	</div>



	<script>
		function getStream() {
			return new Promise((resolve, reject) => {
				let canvas = document.getElementById("SoundBowJS");
				var audioContext = new AudioContext();
				var dest = audioContext.createMediaStreamDestination();
				
				// Add each audio wall to stream
				for (var i = 0; i < 8; i++) {
					let addedStream1 = audioElementArrayPentascale[i].captureStream();
					let addedStream2 = audioElementArrayHarmonicscale[i].captureStream();
					audioIn_01 = audioContext.createMediaStreamSource(addedStream1);
					audioIn_02 = audioContext.createMediaStreamSource(addedStream2);



					audioIn_01.connect(dest);
					audioIn_02.connect(dest);


				}
				dest.stream.addTrack(canvas.captureStream(30).getTracks()[0]);

				resolve(dest.stream);
			})
		}

			let preview = document.getElementById("preview");
			let recording = document.getElementById("recording");
			let startButton = document.getElementById("startButton");
			let stopButton = document.getElementById("stopButton");
			let downloadButton = document.getElementById("downloadButton");
			let logElement = document.getElementById("log");

			let recordingTimeMS = 5000;

function log(msg) {
				logElement.innerHTML += msg + "\n";
}

function wait(delayInMS) {
	  return new Promise(resolve => setTimeout(resolve, delayInMS));
}




function stopButtonClick()
{
	console.log("stop button clicked");
	stopButton.click();
}

function startButtonClick()
{
	console.log("start button clicked");
	startButton.click();	
}

function startRecording(stream, lengthInMS) {
	  let recorder = new MediaRecorder(stream);
	  let data = [];

	  recorder.ondataavailable = event => 
	  	{
		  data.push(event.data);

		}	
	  recorder.start();

	  wait(1000).then(() => canPlay = true)

	  log(recorder.state + " for " + (lengthInMS/1000) + " seconds...");

	  let stopped = new Promise((resolve, reject) => {
				recorder.onstop = resolve;
		recorder.onerror = event => reject(event.name);
	  });



	  let recorded = wait(lengthInMS).then(
		() => recorder.state == "recording" && recorder.stop()
	  );
	  


	  return Promise.all([
		stopped,
		recorded
	  ])
	  .then(() => data);
}

function stop(stream) {
				if (stream)
				{
				stream.getTracks().forEach(track => track.stop());
	}
}

// Global variable, async is not working with processing 
// so had to go trough it in some way
var recordingInProgress = false;
var startRecordingBool = false;
var canPlay = false;

function getCanPlay()
{
	if(canPlay){
		canPlay = false;
		return true;
	}
	return false;
}

function getRecordingInProgress()
{
	return recordingInProgress;
}

function getStartRecording()
{
	if(startRecordingBool)
	{
		startRecordingBool = false;
		return true;
	}
	return false;
}

startButton.addEventListener("click", function() {

					// Stop previous audios
  					for(i=0; i<audioElementArrayPentascale.length; i++) audioElementArrayPentascale[i].pause();
					for(i=0; i<audioElementArrayHarmonicscale.length; i++) audioElementArrayHarmonicscale[i].pause();
					recordingInProgress = true;
				

				getStream().then(stream => {
					preview.srcObject = stream;
					downloadButton.href = stream;
					preview.captureStream = preview.captureStream || preview.mozCaptureStream;
					return new Promise(resolve => preview.onplaying = resolve);
				}).then(() => startRecording(preview.captureStream(), recordingTimeMS))
					.then(recordedChunks => {
						let recordedBlob = new Blob(recordedChunks, { type: "video/mp4" });
						recording.src = URL.createObjectURL(recordedBlob);
						downloadButton.href = recording.src;
						downloadButton.download = "RecordedSound.mp4";
						downloadButton.click();
						log("Successfully recorded " + recordedBlob.size + " bytes of " +
							recordedBlob.type + " media.");
						console.log("recorder" + recordedBlob.size);
						recordingInProgress = false;

						
						
					})
					.catch(log);
}, false);

stopButton.addEventListener("click", function() {
				if(preview.srcObject)
				{
					return;
				}
				stop(preview.srcObject);
			
				
}, false);

	</script>
</head>
	
		<body>
		<div id="content">
			<button id="sendButton">
				<img src="./assets/pix/record_btn.png" alt="RECORD" >

			</button>
			<script>
				let sendButton = document.getElementById("sendButton");
				sendButton.addEventListener("click", function() {
					startRecordingBool = true;
				}, false)
			</script>
			<div>
				<canvas id="SoundBowJS" data-processing-sources="./assets/code/SoundBow_JS.pde" width="1000" height="600">
					<p>Your browser does not support the canvas tag.</p>
				</canvas>
				<noscript>
					<p>JavaScript is required to view the contents of this page.</p>
				</noscript>
	    	</div>
			<h1>SoundBow_JS</h1>
		</div>
	</body>

</html>
